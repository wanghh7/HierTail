import utilsimport mathimport torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.nn.parameter import Parameterfrom torch.nn.modules.module import Modulefrom torch.nn import BatchNorm1d, LeakyReLU, Linearimport torch_sparsefrom torch_scatter import scatter_max, scatter_addfrom torch_geometric.nn.pool.topk_pool import topk, filter_adjfrom torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmpfrom torch_geometric.nn import GCNConv, GATConv, SAGEConv, JumpingKnowledge, DeepGCNLayer, DenseGraphConv, DenseSAGEConv, dense_diff_pool, dense_mincut_pool, MemPooling, TopKPoolingfrom torch_geometric.nn.conv.le_conv import LEConvfrom torch_geometric.utils import to_dense_adj, to_dense_batch, add_remaining_self_loops, add_self_loops, remove_self_loops, sort_edge_index, softmaxfrom torch_geometric.utils.repeat import repeat#--------------### layers####--------------class GraphConv(Module):    """    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907    """    def __init__(self, in_features, out_features, bias=True):        super(GraphConv, self).__init__()        self.in_features = in_features        self.out_features = out_features        self.weight = Parameter(torch.FloatTensor(in_features, out_features))        if bias:            self.bias = Parameter(torch.FloatTensor(out_features))        else:            self.register_parameter('bias', None)        self.reset_parameters()    def reset_parameters(self):        stdv = 1. / math.sqrt(self.weight.size(1))        self.weight.data.uniform_(-stdv, stdv)        if self.bias is not None:            self.bias.data.uniform_(-stdv, stdv)    def forward(self, input, adj):        support = torch.mm(input, self.weight)        output = torch.mm(adj, support)        #for 3_D batch, need a loop!!!        if self.bias is not None:            return output + self.bias        else:            return outputclass GraphAttConv(nn.Module):    def __init__(self, in_features, out_features, heads, dropout):        super(GraphAttConv, self).__init__()        assert out_features % heads == 0        out_perhead = out_features // heads        self.graph_atts = nn.ModuleList([GraphAttConvOneHead(               in_features, out_perhead, dropout=dropout) for _ in range(heads)])        self.in_features = in_features        self.out_perhead = out_perhead        self.heads = heads    def forward(self, input, adj):        output = torch.cat([att(input, adj) for att in self.graph_atts], dim=1)        # notice that original GAT use elu as activation func.         return output    def __repr__(self):        return self.__class__.__name__ + "({}->[{}x{}])".format(                    self.in_features, self.heads, self.out_perhead)   class GraphAttConvOneHead(nn.Module):    """    Sparse version GAT layer, single head    """    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2):        super(GraphAttConvOneHead, self).__init__()        self.weight = Parameter(torch.zeros(size=(in_features, out_features)))        self.a = Parameter(torch.zeros(size=(1, 2*out_features)))        # init         nn.init.xavier_normal_(self.weight.data, gain=nn.init.calculate_gain('relu')) # look at here        nn.init.xavier_normal_(self.a.data, gain=nn.init.calculate_gain('relu'))        self.dropout = nn.Dropout(dropout)        self.leakyrelu = nn.LeakyReLU(alpha)             def forward(self, input, adj):        edge = adj._indices()        h = torch.mm(input, self.weight)        # Self-attention on the nodes - Shared attention mechanism        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t() # edge_h: 2*D x E        # do softmax for each row, this need index of each row, and for each row do softmax over it        alpha = self.leakyrelu(self.a.mm(edge_h).squeeze()) # E        n = len(input)        alpha = self.softmax(alpha, edge[0], n)        output = torch_sparse.spmm(edge, self.dropout(alpha), n, n, h) # h_prime: N x out        # output = torch_sparse.spmm(edge, self.dropout(alpha), n, n, self.dropout(h)) # h_prime: N x out        return output    def softmax(self, src, index, num_nodes=None):        """        sparse softmax        """        num_nodes = index.max().item() + 1 if num_nodes is None else num_nodes        out = src - scatter_max(src, index, dim=0, dim_size=num_nodes)[0][index]        out = out.exp()        out = out / (scatter_add(out, index, dim=0, dim_size=num_nodes)[index] + 1e-16)        return out#--------------### models ####--------------class GNN_Encoder(nn.Module):    def __init__(self, layer, nfeat, nhid, dropout, nhead=1, adj=None):        super(GNN_Encoder, self).__init__()        if layer == 'gcn':            self.conv = GraphConv(nfeat, nhid)            self.activation = nn.ReLU()        elif layer == 'gat':            self.conv = GraphAttConv(nfeat, nhid, nhead, dropout)            self.activation = nn.ELU()                self.dropout = nn.Dropout(p=dropout)        self.adj = adj    def forward(self, x, adj=None):        if adj == None:            adj = self.adj        x = self.activation(self.conv(x, adj))        output = self.dropout(x)        return outputclass GNN_Classifier(nn.Module):    def __init__(self, layer, nhid, nclass, dropout, nhead=1, adj=None):        super(GNN_Classifier, self).__init__()        if layer == 'gcn':            self.conv = GraphConv(nhid, nhid)            self.activation = nn.ReLU()        elif layer == 'gat':            self.conv = GraphAttConv(nhid, nhid, nhead, dropout)            self.activation = nn.ELU(True)                self.mlp = nn.Linear(nhid, nclass)        self.dropout = nn.Dropout(p=dropout)        self.adj = adj        self.reset_parameters()    def reset_parameters(self):        nn.init.normal_(self.mlp.weight,std=0.05)    def forward(self, x, adj=None, logit=False):        if adj == None:            adj = self.adj        x = self.activation(self.conv(x, adj))        x = self.dropout(x)        if logit:            return x        x = self.mlp(x)                return xclass MLP(nn.Module):    def __init__(self, nhid, nclass):        super(MLP, self).__init__()        self.mlp = nn.Linear(nhid, nclass)        self.reset_parameters()    def reset_parameters(self):        nn.init.normal_(self.mlp.weight,std=0.05)    def forward(self, x):        x = self.mlp(x)        return xclass Decoder(nn.Module):    """    Edge Reconstruction adopted in GraphSMOTE (https://arxiv.org/abs/2103.08826)    """    def __init__(self, nhid, dropout=0.1):        super(Decoder, self).__init__()        self.dropout = dropout        self.de_weight = Parameter(torch.FloatTensor(nhid, nhid))        self.reset_parameters()    def reset_parameters(self):        stdv = 1. / math.sqrt(self.de_weight.size(1))        self.de_weight.data.uniform_(-stdv, stdv)    def forward(self, node_embed):        combine = F.linear(node_embed, self.de_weight)        adj_out = torch.sigmoid(torch.mm(combine, combine.transpose(-1,-2)))        return adj_outclass DP_Block(nn.Module):    """The differentiable pooling operator from the    "Hierarchical Graph Representation Learning with Differentiable Pooling" <https://arxiv.org/abs/1806.08804> paper.    Code: https://github.com/pyg-team/pytorch_geometric/blob/66b17806b1f4a2008e8be766064d9ef9a883ff03/benchmark/kernel/diff_pool.py    """    def __init__(self, in_channels, hidden_channels, out_channels, mode='cat'):        super().__init__()        self.conv1 = DenseSAGEConv(in_channels, hidden_channels)        self.conv2 = DenseSAGEConv(hidden_channels, out_channels)        self.jump = JumpingKnowledge(mode)        if mode == 'cat':            self.lin = Linear(hidden_channels + out_channels, out_channels)        else:            self.lin = Linear(out_channels, out_channels)    def reset_parameters(self):        self.conv1.reset_parameters()        self.conv2.reset_parameters()        self.lin.reset_parameters()    def forward(self, x, adj, mask=None):        x1 = F.relu(self.conv1(x, adj, mask))        x2 = F.relu(self.conv2(x1, adj, mask))        return self.lin(self.jump([x1, x2]))class DiffPool(nn.Module):    """    Code: https://github.com/pyg-team/pytorch_geometric/blob/66b17806b1f4a2008e8be766064d9ef9a883ff03/benchmark/kernel/diff_pool.py    """    def __init__(self, num_features, num_classes, hidden, num_layers, ratio=0.25):        super().__init__()        # num_nodes = math.ceil(ratio * num_classes)        num_nodes = math.ceil(num_classes * 4)        self.embed_block1 = DP_Block(num_features, hidden, hidden)        self.pool_block1 = DP_Block(num_features, hidden, num_nodes)        self.embed_blocks = torch.nn.ModuleList()        self.pool_blocks = torch.nn.ModuleList()        for i in range(num_layers - 1):            num_nodes = math.ceil(ratio * num_nodes)            self.embed_blocks.append(DP_Block(hidden, hidden, hidden))            self.pool_blocks.append(DP_Block(hidden, hidden, num_nodes))        self.lin1 = Linear(hidden, hidden)        # self.lin1 = Linear((len(self.embed_blocks) + 1) * hidden, hidden)        self.lin2 = Linear(hidden, num_classes)    def reset_parameters(self):        self.embed_block1.reset_parameters()        self.pool_block1.reset_parameters()        for embed_block, pool_block in zip(self.embed_blocks,                                           self.pool_blocks):            embed_block.reset_parameters()            pool_block.reset_parameters()        self.lin1.reset_parameters()        self.lin2.reset_parameters()    def forward(self, x, adj):        mask, l_s, l_x, l_embed, loss_ = None, [], [], [], 0        s = self.pool_block1(x, adj, mask)        l_s.append(torch.squeeze(s))        x = F.relu(self.embed_block1(x, adj, mask))        l_x.append(torch.squeeze(x))        x, adj, link_loss, ent_loss = dense_diff_pool(x, adj, s, mask)        loss_ += (link_loss + ent_loss)        for i, (embed_block, pool_block) in enumerate(                zip(self.embed_blocks, self.pool_blocks)):            s = pool_block(x, adj)            l_s.append(torch.mm(l_s[-1], torch.squeeze(s)))            x = F.relu(embed_block(x, adj))            l_x.append(torch.squeeze(x))            if i < len(self.embed_blocks) - 1:                x, adj, link_loss, ent_loss = dense_diff_pool(x, adj, s)                loss_ += (link_loss + ent_loss)        l_embed.append(l_x[0])        for i in range(len(l_x) - 1):            l_embed.append(torch.mm(l_s[i], l_x[i + 1]))        # embed = torch.cat(l_embed, 1)        embed = sum(l_embed)        embed = F.relu(self.lin1(embed))        embed = F.dropout(embed, p=0.5, training=self.training)        embed = self.lin2(embed)        return embed, loss_class Mincut_Pool(nn.Module):    """The MinCut pooling operator from the    "Spectral Clustering in Graph Neural Networks for Graph Pooling" <https://arxiv.org/abs/1907.00481> paper.    Code: https://github.com/pyg-team/pytorch_geometric/blob/f5d7ca69e324be1fc25ebca76a8d874019d8cab5/examples/proteins_mincut_pool.py    """    def __init__(self, in_channels, out_channels, hidden_channels):        super().__init__()        self.conv1 = GCNConv(in_channels, hidden_channels)        num_nodes = out_channels * 4        self.pool1 = Linear(hidden_channels, num_nodes)        self.conv2 = DenseGraphConv(hidden_channels, hidden_channels)        num_nodes = math.ceil(0.25 * num_nodes)        self.pool2 = Linear(hidden_channels, num_nodes)        self.conv3 = DenseGraphConv(hidden_channels, hidden_channels)        # self.lin1 = Linear(hidden_channels * 3, hidden_channels)        self.lin1 = Linear(hidden_channels, hidden_channels)        self.lin2 = Linear(hidden_channels, out_channels)    def forward(self, x, edge_index, batch=None):        l_s, l_x, l_embed = [], [], []        x = self.conv1(x, edge_index).relu()        l_x.append(torch.squeeze(x))        x, mask = to_dense_batch(x, batch)        adj = to_dense_adj(edge_index, batch)        s = self.pool1(x)        max_mask = (torch.squeeze(s) == torch.squeeze(s).max(dim=1, keepdim=True)[0])        l_s.append(torch.mul(max_mask, torch.squeeze(s)))        x, adj, mc1, o1 = dense_mincut_pool(x, adj, s, mask)        x = self.conv2(x, adj).relu()        l_x.append(torch.squeeze(x))        s = self.pool2(x)        tmp = torch.mm(l_s[-1], torch.squeeze(s))        max_mask = (tmp == tmp.max(dim=1, keepdim=True)[0])        l_s.append(torch.mul(max_mask, tmp))        x, adj, mc2, o2 = dense_mincut_pool(x, adj, s)        x = self.conv3(x, adj)        l_x.append(torch.squeeze(x))        # l_embed.append(l_x[0])        # for i in range(len(l_x) - 1):        #     l_embed.append(torch.mm(l_s[i], l_x[i + 1]))        embed = l_x[0] + 0.2 * torch.mm(l_s[0], l_x[1]) + 0.1 * torch.mm(l_s[1], l_x[2])        # embed = torch.cat(l_embed, 1)        embed = self.lin1(embed).relu()        embed = self.lin2(embed)        return embed, mc1 + mc2, o1 + o2class Mem_Pool(torch.nn.Module):    """Memory based pooling layer from    "Memory-Based Graph Networks" <https://arxiv.org/abs/2002.09518> paper.    Code: https://github.com/pyg-team/pytorch_geometric/blob/66b17806b1f4a2008e8be766064d9ef9a883ff03/examples/mem_pool.py    """    def __init__(self, in_channels, hidden_channels, out_channels, dropout):        super().__init__()        self.dropout = dropout        self.lin = Linear(in_channels, hidden_channels)        self.convs = torch.nn.ModuleList()        for i in range(2):            conv = GATConv(hidden_channels, hidden_channels, dropout=dropout)            norm = BatchNorm1d(hidden_channels)            act = LeakyReLU()            self.convs.append(                DeepGCNLayer(conv, norm, act, block='res+', dropout=dropout))        self.mem1 = MemPooling(hidden_channels, 16, heads=5, num_clusters=70)        self.mem2 = MemPooling(16, 16, heads=5, num_clusters=35)        self.lin2 = Linear(16, out_channels)    def forward(self, x, edge_index, batch=None):        l_x, l_embed = [], []        x = self.lin(x)        for conv in self.convs:            x = conv(x, edge_index)        l_x.append(torch.squeeze(x))        x, S1 = self.mem1(x, batch)        x = F.leaky_relu(x)        x = F.dropout(x, p=self.dropout)        l_x.append(torch.squeeze(x))        x, S2 = self.mem2(x)        l_x.append(torch.squeeze(x))        # l_embed.append(l_x[0])        # l_embed.append(torch.mm(torch.squeeze(S1), l_x[1]))        # l_embed.append(torch.mm(torch.mm(torch.squeeze(S1), torch.squeeze(S2)), l_x[2]))        # embed = torch.cat(l_embed, 1)        # embed = self.lin2(embed)        embed = l_x[0] + torch.mm(torch.squeeze(S1), l_x[1]) + torch.mm(torch.mm(torch.squeeze(S1), torch.squeeze(S2)), l_x[2])        embed = self.lin2(embed)        return embed, MemPooling.kl_loss(S1) + MemPooling.kl_loss(S2)class ASAP_Pooling(nn.Module):    """The ASAP Pooling from the "ASAP: Adaptive Structure Aware Pooling for Learning Hierarchical Graph Representations"    <https://arxiv.org/abs/1911.07979> paper.    Code: https://github.com/malllabiisc/ASAP/blob/a93de7a41c2bcfcbe0c9faece41ce18a2c4a6087/asap_pool.py    """    def __init__(self, in_channels, ratio, dropout_att=0, negative_slope=0.2):        super(ASAP_Pooling, self).__init__()        self.in_channels = in_channels        self.ratio = ratio        self.negative_slope = negative_slope        self.dropout_att = dropout_att        self.lin_q = Linear(in_channels, in_channels)        self.gat_att = Linear(2 * in_channels, 1)        self.gnn_score = LEConv(self.in_channels, 1)  # gnn_score: uses LEConv to find cluster fitness scores        self.gnn_intra_cluster = GCNConv(self.in_channels, self.in_channels)  # gnn_intra_cluster: uses GCN to account for intra cluster properties, e.g., edge-weights        self.reset_parameters()    def reset_parameters(self):        self.lin_q.reset_parameters()        self.gat_att.reset_parameters()        self.gnn_score.reset_parameters()        self.gnn_intra_cluster.reset_parameters()    def forward(self, x, edge_index, edge_weight=None, batch=None):        if batch is None:            batch = edge_index.new_zeros(x.size(0))        # NxF        x = x.unsqueeze(-1) if x.dim() == 1 else x        # Add Self Loops        fill_value = 1        num_nodes = scatter_add(batch.new_ones(x.size(0)), batch, dim=0)        edge_index, edge_weight = add_remaining_self_loops(edge_index=edge_index, edge_attr=edge_weight,                                                           fill_value=fill_value, num_nodes=num_nodes.sum())        N = x.size(0)  # total num of nodes in batch        # ExF        x_pool = self.gnn_intra_cluster(x=x, edge_index=edge_index, edge_weight=edge_weight)        x_pool_j = x_pool[edge_index[1]]        x_j = x[edge_index[1]]        # ---Master query formation---        # NxF        X_q, _ = scatter_max(x_pool_j, edge_index[0], dim=0)        # NxF        M_q = self.lin_q(X_q)        # ExF        M_q = M_q[edge_index[0].tolist()]        score = self.gat_att(torch.cat((M_q, x_pool_j), dim=-1))        score = F.leaky_relu(score, self.negative_slope)        score = softmax(score, edge_index[0], num_nodes=num_nodes.sum())        # Sample attention coefficients stochastically.        score = F.dropout(score, p=self.dropout_att, training=self.training)        # ExF        v_j = x_j * score.view(-1, 1)        # ---Aggregation---        # NxF        out = scatter_add(v_j, edge_index[0], dim=0)        # ---Cluster Selection        # Nx1        fitness = torch.sigmoid(self.gnn_score(x=out, edge_index=edge_index)).view(-1)        perm = topk(x=fitness, ratio=self.ratio, batch=batch)        x = out[perm] * fitness[perm].view(-1, 1)        # ---Maintaining Graph Connectivity        batch = batch[perm]        edge_index, edge_weight = utils.graph_connectivity(            device=x.device,            perm=perm,            edge_index=edge_index,            edge_weight=edge_weight,            score=score,            ratio=self.ratio,            batch=batch,            N=N)        return x, edge_index, edge_weight, batch, permclass ASAP_Pool(nn.Module):    """    Code: https://github.com/malllabiisc/ASAP/blob/a93de7a41c2bcfcbe0c9faece41ce18a2c4a6087/asap_pool_model.py#L14    """    def __init__(self, num_features, num_classes, num_layers, hidden, ratio=0.8, **kwargs):        super(ASAP_Pool, self).__init__()        if type(ratio)!=list:            ratio = [ratio for i in range(num_layers)]        self.conv1 = GCNConv(num_features, hidden)        self.pool1 = ASAP_Pooling(in_channels=hidden, ratio=ratio[0], **kwargs)        self.convs = torch.nn.ModuleList()        self.pools = torch.nn.ModuleList()        for i in range(num_layers - 1):            self.convs.append(GCNConv(hidden, hidden))            self.pools.append(ASAP_Pooling(in_channels=hidden, ratio=ratio[i], **kwargs))        self.lin1 = Linear(hidden, hidden) # 2*hidden due to readout layer        self.lin2 = Linear(hidden, num_classes)        self.reset_parameters()    def reset_parameters(self):        self.conv1.reset_parameters()        self.pool1.reset_parameters()        for conv, pool in zip(self.convs, self.pools):            conv.reset_parameters()            pool.reset_parameters()        self.lin1.reset_parameters()        self.lin2.reset_parameters()    def forward(self, x, edge_index, batch=None):        l_x = []        x = F.relu(self.conv1(x, edge_index))        l_x.append(x)        x, edge_index, edge_weight, batch, perm = self.pool1(x=x, edge_index=edge_index, edge_weight=None, batch=batch)        for conv, pool in zip(self.convs, self.pools):            x = F.relu(conv(x=x, edge_index=edge_index, edge_weight=edge_weight))            tmp = torch.zeros_like(l_x[0])            tmp[perm] = x            l_x.append(tmp)            x, edge_index, edge_weight, batch, perm = pool(x=x, edge_index=edge_index, edge_weight=edge_weight, batch=batch)        embed = F.relu(self.lin1(sum(l_x)))        embed = F.dropout(embed, p=0.5, training=self.training)        embed = self.lin2(embed)        return embedclass SAGPool_Net(nn.Module):    """The self-attention pooling operator from the    "Self-Attention Graph Pooling" <https://arxiv.org/abs/1904.08082> and    "Understanding Attention and Generalization in Graph Neural Networks" <https://arxiv.org/abs/1905.02850> papers    Code: https://github.com/inyeoplee77/SAGPool/blob/master/layers.py    """    def __init__(self, in_channels, ratio=0.8, Conv=GCNConv, non_linearity=torch.tanh):        super(SAGPool_Net, self).__init__()        self.in_channels = in_channels        self.ratio = ratio        self.score_layer = Conv(in_channels,1)        self.non_linearity = non_linearity    def forward(self, x, edge_index, edge_attr=None, batch=None):        if batch is None:            batch = edge_index.new_zeros(x.size(0))        #x = x.unsqueeze(-1) if x.dim() == 1 else x        score = self.score_layer(x,edge_index).squeeze()        perm = topk(score, self.ratio, batch)        x = x[perm] * self.non_linearity(score[perm]).view(-1, 1)        batch = batch[perm]        edge_index, edge_attr = filter_adj(            edge_index, edge_attr, perm, num_nodes=score.size(0))        return x, edge_index, edge_attr, batch, permclass SAGPool(nn.Module):    """    Code: https://github.com/inyeoplee77/SAGPool/blob/master/networks.py    """    def __init__(self, nfeat, nhid, nclass, pooling_ratio):        super(SAGPool, self).__init__()        self.conv1 = GCNConv(nfeat, nhid)        self.pool1 = SAGPool_Net(in_channels=nhid, ratio=pooling_ratio)        self.conv2 = GCNConv(nhid, nhid)        self.pool2 = SAGPool_Net(in_channels=nhid, ratio=pooling_ratio)        self.conv3 = GCNConv(nhid, nhid)        self.pool3 = SAGPool_Net(in_channels=nhid, ratio=pooling_ratio)        self.lin1 = torch.nn.Linear(nhid * 2, nhid)        self.lin2 = torch.nn.Linear(nhid, nhid // 2)        self.lin3 = torch.nn.Linear(nhid // 2, nclass)    def forward(self, x, edge_index, dropout):        batch = None        x = F.relu(self.conv1(x, edge_index))        x, edge_index, _, batch, _ = self.pool1(x, edge_index, None, batch)        x1 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)        x = F.relu(self.conv2(x, edge_index))        x, edge_index, _, batch, _ = self.pool2(x, edge_index, None, batch)        x2 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)        x = F.relu(self.conv3(x, edge_index))        x, edge_index, _, batch, _ = self.pool3(x, edge_index, None, batch)        x3 = torch.cat([gmp(x, batch), gap(x, batch)], dim=1)        x = x1 + x2 + x3        x = F.relu(self.lin1(x))        x = F.dropout(x, p=dropout, training=self.training)        x = F.relu(self.lin2(x))        x = F.log_softmax(self.lin3(x), dim=-1)        return xclass Vanilla_GCN(nn.Module):    def __init__(self, in_channels, channels, dropout, act, improved=True):        super().__init__()        self.act = act        self.dropout = dropout        self.conv1 = GCNConv(in_channels, channels, improved)        self.conv2 = GCNConv(channels, channels, improved)    def reset_parameters(self):        self.conv1.reset_parameters()        self.conv2.reset_parameters()    def forward(self, x, edge_index, edge_weight=None):        x = self.conv1(x, edge_index, edge_weight)        x = self.act(x)        x = F.dropout(x, p=self.dropout, training=self.training)        x = self.conv2(x, edge_index, edge_weight)        return xclass GraphUNet(torch.nn.Module):    """The Graph U-Net model from the "Graph U-Nets" <https://arxiv.org/abs/1905.05178> paper    which implements a U-Net like architecture with graph pooling and unpooling operations.    Code: https://github.com/pyg-team/pytorch_geometric/blob/c2c11668161a92a7f85cf85bc4df4baa22c68556/torch_geometric/nn/models/graph_unet.py    """    def __init__(self, in_channels, hidden_channels, out_channels, depth, gnn_type, pool_ratios=0.5, act=F.relu, dropout=0.0, sum_res=True):        super().__init__()        assert depth >= 1        self.in_channels = in_channels        self.hidden_channels = hidden_channels        self.out_channels = out_channels        self.depth = depth        self.pool_ratios = repeat(pool_ratios, depth)        self.act = act        self.sum_res = sum_res        channels = hidden_channels        if gnn_type == 'gcn2':            gnn_layer1 = Vanilla_GCN(in_channels, channels, dropout, act, improved=True)            gnn_layer2 = Vanilla_GCN(channels, channels, dropout, act, improved=True)        else:            gnn_layer1 = GCNConv(in_channels, channels, improved=True)            gnn_layer2 = GCNConv(channels, channels, improved=True)        self.down_convs = torch.nn.ModuleList()        self.pools = torch.nn.ModuleList()        self.down_convs.append(gnn_layer1)        for i in range(depth):            self.pools.append(TopKPooling(channels, self.pool_ratios[i]))            self.down_convs.append(gnn_layer2)        in_channels = channels if sum_res else 2 * channels        if gnn_type == 'gcn2':            gnn_layer3 = Vanilla_GCN(in_channels, channels, dropout, act, improved=True)            gnn_layer4 = Vanilla_GCN(in_channels, out_channels, dropout, act, improved=True)        else:            gnn_layer3 = GCNConv(in_channels, channels, improved=True)            gnn_layer4 = GCNConv(in_channels, out_channels, improved=True)        self.up_convs = torch.nn.ModuleList()        for i in range(depth - 1):            self.up_convs.append(gnn_layer3)        self.up_convs.append(gnn_layer4)        self.reset_parameters()    def reset_parameters(self):        for conv in self.down_convs:            conv.reset_parameters()        for pool in self.pools:            pool.reset_parameters()        for conv in self.up_convs:            conv.reset_parameters()    def forward(self, x, edge_index, batch=None):        if batch is None:            batch = edge_index.new_zeros(x.size(0))        edge_weight = x.new_ones(edge_index.size(1))        x = self.down_convs[0](x, edge_index, edge_weight)        x = self.act(x)        xs = [x]        edge_indices = [edge_index]        edge_weights = [edge_weight]        perms = []        for i in range(1, self.depth + 1):            edge_index, edge_weight = self.augment_adj(edge_index, edge_weight, x.size(0))            x, edge_index, edge_weight, batch, perm, _ = self.pools[i - 1](x, edge_index, edge_weight, batch)            x = self.down_convs[i](x, edge_index, edge_weight)            x = self.act(x)            if i < self.depth:                xs += [x]                edge_indices += [edge_index]                edge_weights += [edge_weight]            perms += [perm]        for i in range(self.depth):            j = self.depth - 1 - i            res = xs[j]            edge_index = edge_indices[j]            edge_weight = edge_weights[j]            perm = perms[j]            up = torch.zeros_like(res)            up[perm] = x            x = res + up if self.sum_res else torch.cat((res, up), dim=-1)            x = self.up_convs[i](x, edge_index, edge_weight)            x = self.act(x) if i < self.depth - 1 else x        return x, xs, perms    def augment_adj(self, edge_index, edge_weight, num_nodes):        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)        edge_index, edge_weight = add_self_loops(edge_index, edge_weight,                                                 num_nodes=num_nodes)        edge_index, edge_weight = sort_edge_index(edge_index, edge_weight,                                                  num_nodes)        edge_index, edge_weight = torch_sparse.spspmm(edge_index, edge_weight, edge_index,                                         edge_weight, num_nodes, num_nodes,                                         num_nodes)        edge_index, edge_weight = remove_self_loops(edge_index, edge_weight)        return edge_index, edge_weight